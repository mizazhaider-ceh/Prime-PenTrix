"""
Prime PenTrix - BM25 Keyword Search Engine
===========================================
Okapi BM25 ranking for keyword-based retrieval.
Catches exact term matches that embedding models sometimes miss.
Zero external dependencies beyond rank-bm25 (50KB).

Ported from S2-Sentinel-Copilot with improvements for Prime PenTrix.

Parameters tuned for academic/technical cybersecurity documents:
- k1=1.5 (term frequency saturation)
- b=0.75 (document length normalization)
"""

import re
import math
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Optional


class BM25Index:
    """
    Okapi BM25 ranking function for keyword-based retrieval.

    Usage:
        index = BM25Index()
        index.add_document("chunk-1", "Subnet masks divide networks...")
        index.add_document("chunk-2", "SQL injection attacks exploit...")
        results = index.search("subnet mask", limit=5)
        # → [("chunk-1", 8.5), ...]
    """

    STOP_WORDS = frozenset([
        "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
        "of", "with", "by", "from", "as", "is", "was", "are", "were", "been",
        "be", "have", "has", "had", "do", "does", "did", "will", "would",
        "could", "should", "may", "might", "must", "shall", "can", "need",
        "this", "that", "these", "those", "it", "its", "they", "them",
        "their", "what", "which", "who", "whom", "when", "where", "why", "how",
        "all", "each", "every", "both", "few", "more", "most", "other",
        "some", "such", "no", "nor", "not", "only", "own", "same", "so",
        "than", "too", "very", "just", "also", "now", "here", "there",
        "about", "into", "over", "after", "below", "between", "under",
        "again", "then", "once", "during", "while", "before", "above",
        "being", "through", "further", "because", "until",
    ])

    def __init__(self, k1: float = 1.5, b: float = 0.75):
        self.k1 = k1
        self.b = b
        self.doc_count = 0
        self.avgdl = 0.0
        self.doc_lengths: Dict[str, int] = {}
        self.doc_freqs: Dict[str, int] = defaultdict(int)
        self.inverted_index: Dict[str, Dict[str, int]] = defaultdict(dict)
        self.doc_texts: Dict[str, str] = {}
        self.doc_metadata: Dict[str, Dict] = {}

    # ── Tokenization ────────────────────────────────────────

    def _tokenize(self, text: str) -> List[str]:
        """Tokenize with normalization, stop word removal."""
        tokens = re.findall(
            r"\b[a-zA-Z0-9][\w\-\.]*[a-zA-Z0-9]\b|\b\w\b",
            text.lower(),
        )
        return [t for t in tokens if t not in self.STOP_WORDS and len(t) > 1]

    # ── Index Management ────────────────────────────────────

    def add_document(
        self,
        doc_id: str,
        text: str,
        metadata: Optional[Dict] = None,
    ) -> None:
        """Add a document to the BM25 index."""
        if not isinstance(text, str) or not text.strip():
            return
        if not isinstance(doc_id, str) or not doc_id.strip():
            return

        tokens = self._tokenize(text)
        self.doc_texts[doc_id] = text
        self.doc_metadata[doc_id] = metadata or {}
        self.doc_lengths[doc_id] = len(tokens)

        term_counts = Counter(tokens)
        seen_terms: set = set()

        for term, freq in term_counts.items():
            self.inverted_index[term][doc_id] = freq
            if term not in seen_terms:
                self.doc_freqs[term] += 1
                seen_terms.add(term)

        self.doc_count += 1
        self.avgdl = (
            sum(self.doc_lengths.values()) / max(self.doc_count, 1)
        )

    def add_documents(
        self,
        documents: List[Tuple[str, str, Optional[Dict]]],
    ) -> int:
        """
        Batch add documents.
        Each tuple: (doc_id, text, metadata).
        Returns count of documents added.
        """
        count = 0
        for doc_id, text, metadata in documents:
            self.add_document(doc_id, text, metadata)
            count += 1
        return count

    def remove_document(self, doc_id: str) -> None:
        """Remove a document from the index."""
        if doc_id not in self.doc_texts:
            return

        tokens = self._tokenize(self.doc_texts[doc_id])
        seen_terms: set = set()

        for token in tokens:
            if token not in seen_terms:
                if (
                    token in self.inverted_index
                    and doc_id in self.inverted_index[token]
                ):
                    del self.inverted_index[token][doc_id]
                    self.doc_freqs[token] = max(
                        0, self.doc_freqs[token] - 1
                    )
                    if not self.inverted_index[token]:
                        del self.inverted_index[token]
                        del self.doc_freqs[token]
                seen_terms.add(token)

        del self.doc_texts[doc_id]
        del self.doc_lengths[doc_id]
        if doc_id in self.doc_metadata:
            del self.doc_metadata[doc_id]

        self.doc_count -= 1
        self.avgdl = (
            sum(self.doc_lengths.values()) / max(self.doc_count, 1)
            if self.doc_count > 0
            else 0
        )

    # ── Search ──────────────────────────────────────────────

    def search(
        self,
        query: str,
        limit: int = 10,
    ) -> List[Tuple[str, float]]:
        """
        Search using BM25 scoring.
        Returns list of (doc_id, score) sorted by relevance.
        """
        if self.doc_count == 0:
            return []

        query_tokens = self._tokenize(query)
        if not query_tokens:
            return []

        scores: Dict[str, float] = defaultdict(float)

        for term in query_tokens:
            if term not in self.inverted_index:
                continue

            df = self.doc_freqs.get(term, 0)
            # BM25 IDF formula
            idf = math.log(
                (self.doc_count - df + 0.5) / (df + 0.5) + 1.0
            )

            for doc_id, tf in self.inverted_index[term].items():
                dl = self.doc_lengths[doc_id]
                # BM25 TF normalization
                tf_norm = (tf * (self.k1 + 1)) / (
                    tf
                    + self.k1 * (1 - self.b + self.b * dl / self.avgdl)
                )
                scores[doc_id] += idf * tf_norm

        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return ranked[:limit]

    # ── Utility ─────────────────────────────────────────────

    def clear(self) -> None:
        """Clear the entire index."""
        self.doc_count = 0
        self.avgdl = 0.0
        self.doc_lengths.clear()
        self.doc_freqs.clear()
        self.inverted_index.clear()
        self.doc_texts.clear()
        self.doc_metadata.clear()

    @property
    def size(self) -> int:
        """Number of documents in the index."""
        return self.doc_count

    def __repr__(self) -> str:
        return (
            f"BM25Index(docs={self.doc_count}, "
            f"terms={len(self.inverted_index)}, "
            f"avgdl={self.avgdl:.1f})"
        )
